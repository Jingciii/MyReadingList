# MyReadingList
Reading list for research papers in Data Science field. **Keep Updating**!

## Motivation

Inspired by [Andrew Ng's advice on building a Machine Learning career](https://blog.usejournal.com/advice-on-building-a-machine-learning-career-and-reading-research-papers-by-prof-andrew-ng-f90ac99a0182), I feel learning from reading in Data Science really necessary. Also, I found it super useful during preparing paper presentation within company teams where I'm doing co-op. I've learned a lot through reading and summarizing and the whole process could have been enjoyable and less painfull if I did not consider it too difficult and high-level. So I decided to make it on a regular basis. 


## Basic Requirements for each Paper

 * I do not want myself end up reading for reading. After finishing one paper, I should at lease have to be able to fill the following template 

```
- What did the author(s) try to accomplish?
- What were the key elements of the approach(s)?
- List the concepts/techniques that are new to me and highlight whatever I feel necessary to put into my skill set if there is any.
- Thoughts and questions
```

 * Pick one paper and write comments/reviews notes in more detail (like a tech post) at least biweekly

*If it's simply answering questions in first bullet, mark as [summary]. If containing second case, mark as [note]* 

## Paper List

- [x] 1. [Describing like Humans: on Diversity in Image Captioning](http://visal.cs.cityu.edu.hk/static/pubs/conf/cvpr19-diversecap.pdf) (have presented to the company teams internally)
- [ ] 2. [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) 
- [x] 3. [The Structural Topic Model and Applied Social Science](https://scholar.princeton.edu/files/bstewart/files/stmnips2013.pdf)
- [ ] 4. [Sentiment Analysis of Movie Review Comments](https://pdfs.semanticscholar.org/cbad/1c16d8270f1f1ecd542518ee933922bd647c.pdf)
- [ ] 5. [Going Deeper with Convolutions](http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)
- [ ] 6. [Class-Balanced Loss Based on Effective Number of Samples](http://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf)
- [ ] 7. [SocialStories: Segmentating Stories within Trending Twitter Topics](https://prakharguptaz.github.io/assets/SocialStories_2016_paper.pdf)[Slides](https://slideplayer.com/slide/12715829/)
- [x] 8. [Responsible Team Players Wanted: an Analysis of Soft Skill Requirements in Job Advertisements](https://arxiv.org/pdf/1810.07781.pdf)
- [ ] 9. (Google AI Blog) [Learning Cross-Modal Temporal Representations from Unlabeled Videos](https://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+blogspot%2FgJZg+(Google+AI+Blog))
- [x] 10. [A Balanced Perspective on Prediction and Inference for Data Science in Industry](https://assets.pubpub.org/ifh11bok/71561833106678.pdf)
- [ ] 11. [Anxious Depression Prediction in Real-time Social Data](https://arxiv.org/ftp/arxiv/papers/1903/1903.10222.pdf)
- [ ] 12. [NGBoost: Natural Gradient Boosting for Probabilistic Prediction](https://arxiv.org/pdf/1910.03225v2.pdf)
- [ ] 13. (Blog) [Are All Kernels Cursed?](https://francisbach.com/cursed-kernels/)
- [ ] 14. [Clust-LDA: Joint Model for Text Mining and Author Group Inference](https://arxiv.org/pdf/1810.02717.pdf)
- [ ] 15. [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf)
- [ ] 16. [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)
- [ ] 17. [SSD: Single Shot MultiBox Detector](https://ai.google/research/pubs/pub44872)
- [ ] 18. (Blog) [NLP's ImageNet Moment has arrive](https://thegradient.pub/nlp-imagenet/)
- [ ] 19. [Evading Real-Time Person Detectors by Adversarial T-shirt](https://arxiv.org/pdf/1910.11099v1.pdf)
- [ ] 20. [Snapshot ENsembles: Train 1, Get M for Free](https://arxiv.org/pdf/1704.00109.pdf)
- [x] 21. [Hierachical Attention Networks for Document Classification](https://www.cc.gatech.edu/~dyang888/docs/naacl16.pdf)
- [x] 22. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- [ ] 23. [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [ ] 24. [Causality in Machine Learning](https://arxiv.org/pdf/1911.10500.pdf)
- [ ] 25. [Batch Normalization: Accelerating Deep Network Training by Reduce Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v3.pdf)
- [ ] 26. [DiffTaiChi: Differentiable Programming for Physical Simulation](https://arxiv.org/pdf/1910.00935.pdf)
- [ ] 27. [Delayed Impact of Fair Machine Learning](https://arxiv.org/pdf/1803.04383.pdf)
- [ ] 28. [The Dark Secrets of BERT](https://text-machine-lab.github.io/blog/2020/bert-secrets/)